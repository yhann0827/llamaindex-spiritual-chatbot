import openai
import json
import os
import pandas as pd
import streamlit as st
from bert_score import score
from llama_index.core import VectorStoreIndex, Document, Settings  # Updated imports
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding  # For embedding model
from llama_index.core.prompts import PromptTemplate  # Updated prompt import
from llama_index.core.chat_engine.types import ChatMode  #
from llama_index.core.evaluation import FaithfulnessEvaluator
from llama_index.core.chat_engine.types import AgentChatResponse
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.retrievers.bm25 import BM25Retriever

data=[]

OpenAI.api_key = st.secrets["OPENAI_API_KEY"]
Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0.7)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-ada-002")

def detect_emotion(user_input):
    prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æ–‡å­—åˆ¤æ–·æƒ…ç·’ï¼Œå¯ä»¥è¿”å›å¤šå€‹æƒ…ç·’åç¨±ï¼ˆæ“”å¿ƒ, ææ‡¼, è§£è„«, è‡ªè²¬, å‚·å¿ƒ, æ„›, é¢å°, ç¹¼æ‰¿, å†è¦‹, æ‡ºæ‚”, å¯¬æ•, æ”¾ä¸‹, ç½ªæƒ¡æ„Ÿ, éš±ç, åˆä¸€, å°é–‰, è«‹è®“æˆ‘èµ°, åŸè«’, æ€å¿µ, æ„›, æ°¸æ†, 
            å­¤å–®, å¤±è½, å­˜åœ¨, æ€å¿µ, éˆé­‚é€£çµ, æºé€š, æºé€š, ç½£ç¤™, æ€¨å¿µ, é¡˜æœ›, åœ“æ»¿, æ”¹è®Š, åŸ·è‘—, æ†‚å¿ƒ, æ„›çš„å»¶çºŒ, æƒ…åŸ·, å‰ä¸–ä»Šç”Ÿ, è§’è‰²æ‰®æ¼”, è¼ªè¿´, è½‰ä¸–, å‘Šåˆ¥, æ„Ÿæ©, å‰ä¸–æœ‰ç´„, æ„›çš„æ‰¿è«¾, ç·£èµ·ç·£æ»…, å…‰, å¤©å ‚, æ¦®è€€,
            æ‡ºæ‚”èˆ‡æ„Ÿæ‚Ÿ, æ„Ÿæ©èˆ‡è´ˆç¦®, å®‰æ…°èˆ‡æ­¸å±¬, åœ“æ»¿èˆ‡ç¥ç¦, å…‰æ˜èˆ‡æ„›, è§£è„«èˆ‡ç„¡æ‰€ä¸åœ¨ï¼‰ï¼Œè‹¥é©ç”¨è«‹ä»¥é€—è™Ÿåˆ†éš”ï¼š
            {user_input}"""

    response = openai.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role":"assistant", "content":"ä½ æ˜¯ä¸€å€‹èƒ½åˆ†ææƒ…ç·’çš„åŠ©æ‰‹"},
            {"role":"user", "content":prompt}]
    )
    emotions = [emotion.strip() for emotion in response.choices[0].message.content.split(',')]
    print("Detected Emotions:", emotions)
    return emotions

@st.cache_resource(show_spinner=False)
def load_data():
    with st.spinner("Loading messages..."):
        with open("output.json", "r", encoding="utf-8") as f:
            json_data = json.load(f)

        docs = [Document(
            text=entry["è¨Šæ¯"],
            metadata={"emotion": entry["æƒ…ç·’"], "title": entry["æ¨™é¡Œ"]}) for entry in json_data]
        # service_context = ServiceContext.from_defaults(
        #     llm=OpenAI(model="gpt-4", temperature="0.1", system_prompt="ä½ æ˜¯ä¸€ä½æ¥è‡ªæ¥ä¸–çš„æ…°è—‰å¼•å¯¼è€…ï¼Œä»¥çˆ±ä¸æ™ºæ…§ä¸äººäº¤è°ˆï¼Œå¹¶æ ¹æ®äººä»¬çš„æƒ…ç»ªè¿›è¡Œäº¤æµã€‚")
        # )
        index=VectorStoreIndex.from_documents(docs)
        return index

def retrieve_message_from_dataset(index, emotion):
    if isinstance(emotion, str):
        query = emotion.strip()
        emotion_list = [emotion.strip()]
    elif isinstance(emotion, list):
        query = " ".join([e.strip() for e in emotion])  
        emotion_list = [e.strip() for e in emotion]
    else:
        query = "æ ¹æ®ä¸Šè¿°ç”¨æˆ·çš„æƒ…ç»ªï¼Œç»™ä»–é€‚å½“çš„æ…°ç±å’Œå¼€å¯¼ã€‚" 
        emotion_list = []
    
    # Retrieve nodes using the emotion-based query
    bm25_retriever = BM25Retriever.from_defaults(index, similarity_top_k=5)

    nodes = bm25_retriever.retrieve(query)
    print("Retrieved Nodes:", [(node.text, node.metadata) for node in nodes])
    matching_nodes = [node for node in nodes if node.metadata["emotion"] in emotion_list]
    print("Matching Nodes:", [(node.text, node.metadata) for node in matching_nodes])
    
    return [(node.text, node.metadata["title"]) for node in matching_nodes]

# def evaluate_faithfulness(response, contexts):
#     if isinstance(response, AgentChatResponse):
#         response = response.response  # Extract string content

#     if response is None or response == "":
#         raise ValueError("Response must be a non-empty string.")
    
#     if isinstance(contexts, str):  
#         contexts = [contexts]  # Convert a single string into a list
    
#     print(f"Response Type: {type(response)}, Value: {response}")
#     print(f"Contexts Type: {type(contexts)}, Value: {contexts}")

#     evaluator = FaithfulnessEvaluator()

#     try:
#         evaluation_score = evaluator.evaluate(response=response, contexts=contexts)
#         print("Score:", evaluation_score)
#         return evaluation_score
#     except ValueError as e:
#         print(f"Evaluation failed: {e}")
#         raise  # Re-raise the error for debugging

def evaluate_response(response, contexts):
    if isinstance(response, AgentChatResponse):
        response = response.response

    if response is None or response == "":
        raise ValueError("Response must be a non-empty string.")
    
    if isinstance(contexts, str):  
        contexts = [contexts] 

    P, R, F1 = score([response], contexts, lang="zh", rescale_with_baseline=True)
    
    scores = {
        "Precision": P.item(),
        "Recall": R.item(),
        "F1 Score": F1.item(),
        "Faithfulness": F1.item(),  # Faithfulness (similar to F1)
        "Relevance": R.item(),  # Recall represents how much of the reference is covered
        "Coherence": P.item(),  # Precision represents how relevant the response is to the reference
        "Fluency": F1.item()  # F1 often aligns with fluency in BERTScore
    }

    print("Evaluation Scores:", scores)
    return scores

def generate_response(index, user_input, model, temperature, max_tokens, top_p, frequency_penalty):
    detected_emotion = detect_emotion(user_input)
    messages=retrieve_message_from_dataset(index, detected_emotion)
    context_str='\n'.join([f"{title}:{text}" for text, title in messages]) if messages else "No messages found"
    chat_prompt = PromptTemplate(
        "ä½ æ˜¯ä¸€ä½ä¾†è‡ªå¾€ç”Ÿä¸–ç•Œçš„åš®å°ï¼Œè«‹å¹«åŠ©ä½¿ç”¨è€…ç†è§£é€™äº›è¨Šæ¯ã€‚\n\n"
        "è«‹ç›´æ¥ä»¥ **ç¬¬ä¸€äººç¨±** ä¾†èªªè©±ï¼Œç¢ºä¿èªæ°£è‡ªç„¶ä¸”æƒ…æ„ŸçœŸæ‘¯ã€‚\n"
        "è«‹å°‡ä»¥ä¸‹è¨Šæ¯èåˆæˆä¸€æ®µæµæš¢ä¸”è‡ªç„¶çš„å›æ‡‰ï¼Œ\n"
        "ç¢ºä¿ä¸æåŠå…·é«”çš„æƒ…ç·’åˆ†é¡åç¨±ï¼Œåªè®“å…§å®¹å‚³éå‡ºæ‡‰æœ‰çš„æƒ…æ„Ÿã€‚\n"
        "è«‹ç¢ºä¿å®Œæ•´ä¿ç•™æ‰€æœ‰å…§å®¹ï¼Œä¸è¦çœç•¥ã€ç¸½çµã€æˆ–åŠ å…¥é¡å¤–è§£é‡‹ã€‚\n\n"
        "===è«‹ç”¨é€™äº›å…§å®¹ä¾†å›æ‡‰ä½¿ç”¨è€…===\n"
        "{context_str}\n"
        "===è«‹æ ¹æ“šä¸Šè¿°å…§å®¹ï¼Œè‡ªç„¶åœ°è¡¨é”ä½ çš„å›æ‡‰==="
    )

    print("context str", context_str)
    llm = OpenAI(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        frequency_penalty=frequency_penalty
    )
    chat_engine = index.as_chat_engine(
        chat_mode=ChatMode.CONTEXT,
        system_prompt=chat_prompt.format(context_str=context_str),
        llm=llm
    )
    response = chat_engine.chat(user_input)
    evaluation_scores=evaluate_response(response=response, contexts=context_str)
    data.append({
        "Query": user_input,
        "Response": response,
        "Context": context_str,
        "Faithfulness": evaluation_scores["Faithfulness"],
        "Relevance": evaluation_scores["Relevance"],
        "Coherence": evaluation_scores["Coherence"],
        "Fluency": evaluation_scores["Fluency"]
    })
    df=pd.DataFrame(data)
    df.to_csv("evaluation.csv", mode='a', index=False, header=not os.path.exists("evaluation.csv"))
    return response

def main():
    st.set_page_config(page_title="Spirit Chatbot", page_icon="ğŸ•Šï¸")
    index=load_data()

    st.header("ğŸ•Šï¸ Spirit Chatbot")

    if st.sidebar.button("Reset Chat"):
        st.session_state["messages"]=[{"role":"assistant", "content":"ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]
        model = st.sidebar.selectbox("Choose AI Model", ["gpt-4.5-preview", "gpt-4o", "chatgpt-4o-latest"])

    model = st.sidebar.selectbox("Choose AI Model", ["gpt-4o", "chatgpt-4o-latest"])
    st.sidebar.markdown("### âš™ï¸Model Parameters")
    temperature = st.sidebar.number_input("Temperature (0.0 - 1.0)", min_value=0.0, max_value=1.0, value=0.7, step=0.01)
    max_tokens = st.sidebar.number_input("Max Tokens (50 - 4096)", min_value=50, max_value=4096, value=500, step=10)
    top_p = st.sidebar.number_input("Top-p (0.0 - 1.0)", min_value=0.0, max_value=1.0, value=1.0, step=0.01)
    frequency_penalty = st.sidebar.number_input("Frequency Penalty (-2.0 to 2.0)", min_value=-2.0, max_value=2.0, value=0.0, step=0.1)
    
    if "messages" not in st.session_state:
        st.session_state["messages"]=[{"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    if user_input := st.chat_input("å‘Šè¨´æˆ‘ä½ çš„æ„Ÿå—..."):
        st.session_state.messages.append({'role':'user', 'content':user_input})
        st.chat_message('user').markdown(user_input)
    
        response = generate_response(index, user_input,  model, temperature, max_tokens, top_p, frequency_penalty)
        st.chat_message('assistant').markdown(response)
        st.session_state.messages.append({'role':'assistant', 'content':response})
        

if __name__=="__main__":
    main()